{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitjiwani/reinforcement-learning/blob/main/rl5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class GridWorld:\n",
        "  def __init__(self, size=4, start = (0,0), goal = (3,3)):\n",
        "    self.size = size\n",
        "    self.start = start\n",
        "    self.goal = goal\n",
        "    self.state = start\n",
        "    self.action = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
        "\n",
        "  def reset(self):\n",
        "    self.state = self.start\n",
        "    return self.state\n",
        "\n",
        "  def step(self, action):\n",
        "    next_state = (self.state[0] + self.action[action][0],\n",
        "                  self.state[1] + self.action[action][1])\n",
        "    if 0 <= next_state[0] < self.size and 0 <= next_state[1] < self.size:\n",
        "      self.state = next_state\n",
        "    reward = 1 if self.state == self.goal else -0.1\n",
        "    done = self.state == self.goal\n",
        "    return self.state, reward, done"
      ],
      "metadata": {
        "id": "XC5hP7BbWpW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Monte Carlo**"
      ],
      "metadata": {
        "id": "oi_8yDDjEdAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def monte_carlo(env, episode=5000, gamma=0.9, epsilon=0.1):\n",
        "    Q = {((i, j), a): 0 for i in range(env.size) for j in range(env.size) for a in range(4)}\n",
        "    returns = {((i, j), a): [] for i in range(env.size) for j in range(env.size) for a in range(4)}\n",
        "    for _ in range(episode):\n",
        "        state = env.reset()\n",
        "        episode = []\n",
        "        while True:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(range(4))\n",
        "            else:\n",
        "                action = max(range(4), key=lambda a: Q[(state, a)])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        G = 0\n",
        "        visited = set()\n",
        "        for state, action, reward in reversed(episode):\n",
        "            G = gamma * G + reward\n",
        "            if (state, action) not in visited:\n",
        "                visited.add((state, action))\n",
        "                returns[(state, action)].append(G)\n",
        "                Q[(state, action)] = np.mean(returns[(state, action)])\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "8XjsJzgDEaOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Temporal Differnce**"
      ],
      "metadata": {
        "id": "L-OkpQauFMKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sarsa(env, episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    Q = {((i, j), a): 0 for i in range(env.size) for j in range(env.size) for a in range(4)}\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(range(4))\n",
        "        else:\n",
        "            action = max(range(4), key=lambda a: Q[(state, a)])\n",
        "        while True:\n",
        "            next_state, reward, done = env.step(action)\n",
        "            if random.random() < epsilon:\n",
        "                next_action = random.choice(range(4))\n",
        "            else:\n",
        "                next_action = max(range(4), key=lambda a: Q[(next_state, a)])\n",
        "            Q[(state, action)] += alpha * (\n",
        "                reward + gamma * Q[(next_state, next_action)] - Q[(state, action)]\n",
        "            )\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            if done:\n",
        "                break\n",
        "    return Q"
      ],
      "metadata": {
        "id": "vAur4T33FKQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def q_learning(env, episodes=5000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "    Q = {((i, j), a): 0 for i in range(env.size) for j in range(env.size) for a in range(4)}\n",
        "    for _ in range(episodes):\n",
        "        state = env.reset()\n",
        "        while True:\n",
        "            if random.random() < epsilon:\n",
        "                action = random.choice(range(4))\n",
        "            else:\n",
        "                action = max(range(4), key=lambda a: Q[(state, a)])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            best_next_q = max(Q[(next_state, a)] for a in range(4))\n",
        "            Q[(state, action)] += alpha * (reward + gamma * best_next_q - Q[(state, action)])\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "    return Q"
      ],
      "metadata": {
        "id": "Xu4MhEK3FQZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Initialize Environment\n",
        "env = GridWorld()\n",
        "\n",
        "#train Monte Carlo\n",
        "Q_mc = monte_carlo(env)\n",
        "\n",
        "#Train with SARSA\n",
        "Q_sarsa = sarsa(env)\n",
        "\n",
        "#Train with Q learning\n",
        "Q_q_learning = q_learning(env)"
      ],
      "metadata": {
        "id": "ZNSLPnWMFSzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Print Sample Q-values\n",
        "\n",
        "print(\"Sample Q-values for Monte Carlo:\", {k: Q_mc[k] for k in list(Q_mc.keys())[:17]})\n",
        "print(\"Sample Q-values for SARSA:\", {k: Q_sarsa[k] for k in list(Q_sarsa.keys())[:17]})\n",
        "print(\"Sample Q-values for Q-learning:\", {k: Q_q_learning[k] for k in list(Q_q_learning.keys())[:17]})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbXhPGyDFUVF",
        "outputId": "6209729d-9607-400b-e07e-d4df5534a405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Q-values for Monte Carlo: {((0, 0), 0): np.float64(0.0963000168336826), ((0, 0), 1): np.float64(0.1296902888395616), ((0, 0), 2): np.float64(0.020337589160000103), ((0, 0), 3): np.float64(0.020052882544594693), ((0, 1), 0): np.float64(0.16640000332856175), ((0, 1), 1): np.float64(0.25584389684000014), ((0, 1), 2): np.float64(0.042687242000000104), ((0, 1), 3): np.float64(0.0968351750000001), ((0, 2), 0): np.float64(-0.2709997424145312), ((0, 2), 1): np.float64(0.4580000000000002), ((0, 2), 2): np.float64(-0.999999973038651), ((0, 2), 3): 0, ((0, 3), 0): np.float64(-0.9999612734804219), ((0, 3), 1): np.float64(0.08001434315539939), ((0, 3), 2): np.float64(-0.9999995363461562), ((0, 3), 3): np.float64(-0.9999833295031642), ((1, 0), 0): np.float64(0.26552576374580134)}\n",
            "Sample Q-values for SARSA: {((0, 0), 0): 0.08991569068500306, ((0, 0), 1): 0.13732403987598207, ((0, 0), 2): -0.002837208823490987, ((0, 0), 3): -0.0025913679190554266, ((0, 1), 0): -0.05930870420369839, ((0, 1), 1): 0.236662633229547, ((0, 1), 2): -0.09970048281743667, ((0, 1), 3): -0.013787572789373743, ((0, 2), 0): -0.06651808873870511, ((0, 2), 1): 0.16678619421134577, ((0, 2), 2): -0.06826964261668111, ((0, 2), 3): -0.06697563702157, ((0, 3), 0): -0.045739, ((0, 3), 1): 0.11928798333620777, ((0, 3), 2): -0.032927547439000004, ((0, 3), 3): -0.038674990000000006, ((1, 0), 0): 0.21667816255652744}\n",
            "Sample Q-values for Q-learning: {((0, 0), 0): 0.1809799999999987, ((0, 0), 1): 0.18097906584903697, ((0, 0), 2): 0.06288194543103065, ((0, 0), 3): 0.06288197685098014, ((0, 1), 0): 0.3121988915144147, ((0, 1), 1): 0.3121999999999987, ((0, 1), 2): 0.06288175389609608, ((0, 1), 3): 0.18097896678119746, ((0, 2), 0): 0.079782332490469, ((0, 2), 1): 0.45799999998859536, ((0, 2), 2): 0.03233413786916679, ((0, 2), 3): 0.10552170292112792, ((0, 3), 0): -0.02817543993705233, ((0, 3), 1): 0.48867120651257046, ((0, 3), 2): -0.04414096509370861, ((0, 3), 3): -0.01355273618273943, ((1, 0), 0): 0.3121999999994976}\n"
          ]
        }
      ]
    }
  ]
}