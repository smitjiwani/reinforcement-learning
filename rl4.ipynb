{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smitjiwani/reinforcement-learning/blob/main/rl4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#install dependencies\n",
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.24.3\n",
        "!pip install gym[classic_control]==0.26.2"
      ],
      "metadata": {
        "id": "j0KizdY7k1m5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "74c043f5-c195-4ae3-9fc2-64e04b594d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.24.3\n",
            "Uninstalling numpy-1.24.3:\n",
            "  Successfully uninstalled numpy-1.24.3\n",
            "Collecting numpy==1.24.3\n",
            "  Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "84b8239c98e44a71afab205dbbd9ed6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym==0.26.2 (from gym[classic_control]==0.26.2)\n",
            "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2->gym[classic_control]==0.26.2) (1.24.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2->gym[classic_control]==0.26.2) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.26.2->gym[classic_control]==0.26.2) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control]==0.26.2)\n",
            "  Using cached pygame-2.1.0.tar.gz (5.8 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import required libraries\n",
        "import gym\n",
        "import numpy as np\n",
        "env = gym.make('FrozenLake-v1')"
      ],
      "metadata": {
        "id": "NZ-LYNtFdDlU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6f5c8a-a89d-414c-9d40-789ae466fdef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the given policy for the given environment using iterative policy evaluation.**\n",
        "\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "OQBH7_4ZWBMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "    V = np.zeros(environment.observation_space.n)\n",
        "\n",
        "    for i in range(int(max_iterations)):\n",
        "        delta = 0\n",
        "        for state in range(environment.observation_space.n):\n",
        "            v = 0\n",
        "            for action, action_probability in enumerate(policy[state]):\n",
        "                for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
        "                    v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
        "\n",
        "            delta = max(delta, np.abs(V[state] - v))\n",
        "            V[state] = v\n",
        "\n",
        "        evaluation_iterations += 1\n",
        "        if delta < theta:\n",
        "            print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
        "            return V\n",
        "\n",
        "    print(f'Maximum iterations reached. Policy evaluation terminated after {evaluation_iterations} iterations.')\n",
        "    return V"
      ],
      "metadata": {
        "id": "4wd7gOrQihh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def one_step_lookahead(environment, state, V, discount_factor):\n",
        "    action_values = np.zeros(environment.action_space.n)\n",
        "    for action in range(environment.action_space.n):\n",
        "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
        "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
        "    return action_values"
      ],
      "metadata": {
        "id": "wBk4EYuIirtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
        "    policy = np.ones([environment.observation_space.n, environment.action_space.n]) / environment.action_space.n\n",
        "    evaluated_policies = 1\n",
        "    for i in range(int(max_iterations)):\n",
        "        stable_policy = True\n",
        "        V = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
        "        for state in range(environment.observation_space.n):\n",
        "            current_action = np.argmax(policy[state])\n",
        "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "            best_action = np.argmax(action_value)\n",
        "            if current_action != best_action:\n",
        "                stable_policy = True\n",
        "                policy[state] = np.eye(environment.action_space.n)[best_action]\n",
        "        evaluated_policies += 1\n",
        "        if stable_policy:\n",
        "            print(f'Evaluated {evaluated_policies} policies.')\n",
        "            return policy, V"
      ],
      "metadata": {
        "id": "sYQa9vp9iwCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "    V = np.zeros(environment.observation_space.n)\n",
        "    for i in range(int(max_iterations)):\n",
        "        delta = 0\n",
        "        for state in range(environment.action_space.n):\n",
        "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "            best_action_value = np.max(action_value)\n",
        "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
        "            V[state] = best_action_value\n",
        "        if delta < theta:\n",
        "            print(f'Value-iteration converged at iteration#{i}.')\n",
        "            break\n",
        "\n",
        "    policy = np.zeros([environment.observation_space.n, environment.action_space.n])\n",
        "    for state in range(environment.observation_space.n):\n",
        "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
        "        best_action = np.argmax(action_value)\n",
        "        policy[state, best_action] = 1.0\n",
        "    return policy, V"
      ],
      "metadata": {
        "id": "8Wb7QdGSi1XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def play_episodes(environment, n_episodes, policy):\n",
        "    wins = 0\n",
        "    total_reward = 0\n",
        "    for episode in range(n_episodes):\n",
        "        terminated = False\n",
        "        state = environment.reset()\n",
        "        while not terminated:\n",
        "            action = np.argmax(policy[state])\n",
        "            next_state, reward, terminated, info = environment.step(action)\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            if terminated and reward == 1.0:\n",
        "                    wins += 1\n",
        "    average_reward = total_reward / n_episodes\n",
        "    return wins, total_reward, average_reward"
      ],
      "metadata": {
        "id": "ALjbBRnHi734"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
        "    \"\"\"\n",
        "    Evaluates a policy given an environment and a discount factor.\n",
        "\n",
        "    Args:\n",
        "        policy: A 2D numpy array representing the policy.\n",
        "        environment: The OpenAI Gym environment.\n",
        "        discount_factor: The discount factor.\n",
        "        theta: The threshold for convergence.\n",
        "        max_iterations: The maximum number of iterations.\n",
        "\n",
        "    Returns:\n",
        "        A numpy array representing the value function.\n",
        "    \"\"\"\n",
        "    V = np.zeros(environment.observation_space.n)\n",
        "    # Initialize evaluation_iterations to 0 before using it\n",
        "    evaluation_iterations = 0\n",
        "\n",
        "    for i in range(int(max_iterations)):\n",
        "        delta = 0\n",
        "        for state in range(environment.observation_space.n):\n",
        "            v = 0\n",
        "            for action, action_probability in enumerate(policy[state]):\n",
        "                for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
        "                    v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
        "\n",
        "            delta = max(delta, np.abs(V[state] - v))\n",
        "            V[state] = v\n",
        "\n",
        "        evaluation_iterations += 1\n",
        "        if delta < theta:\n",
        "            print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
        "            return V\n",
        "\n",
        "    print(f'Maximum iterations reached. Policy evaluation terminated after {evaluation_iterations} iterations.')\n",
        "    return V"
      ],
      "metadata": {
        "id": "JcrybWQG9YGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 1000\n",
        "policy, V = policy_iteration(env)\n",
        "wins, total_reward, average_reward = play_episodes(env, n_episodes, policy)\n",
        "print(f'Wins: {wins}/{n_episodes}')"
      ],
      "metadata": {
        "id": "qsOjwQM3Cgqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d7e471-fd6e-44af-9974-437b8112bdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluated in 65 iterations.\n",
            "Evaluated 2 policies.\n",
            "Wins: 714/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 1000\n",
        "\n",
        "solvers = [('Policy Iteration', policy_iteration),\n",
        "           ('Value Iteration', value_iteration)]\n",
        "\n",
        "for iteration_name, iteration_func in solvers:\n",
        "\n",
        "    environment = gym.make('FrozenLake-v1')\n",
        "\n",
        "    policy, V = iteration_func(environment.env)\n",
        "    wins, total_reward, average_reward = play_episodes(env, n_episodes, policy)\n",
        "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
        "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward}\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-KU6HHi0FGV",
        "outputId": "f2205e38-bc0c-4d66-e4a4-d3bf3a8138a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy evaluated in 65 iterations.\n",
            "Evaluated 2 policies.\n",
            "Policy Iteration :: number of wins over 1000 episodes = 727\n",
            "Policy Iteration :: average reward over 1000 episodes = 0.727\n",
            "\n",
            "\n",
            "Value-iteration converged at iteration#0.\n",
            "Value Iteration :: number of wins over 1000 episodes = 0\n",
            "Value Iteration :: average reward over 1000 episodes = 0.0\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}